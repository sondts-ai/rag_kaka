# [File: src/base/llm_model.py]
from langchain_community.llms import Ollama

def get_ollama_llm(model_name: str = "llama3.2", base_url: str = "http://localhost:11434", **kwargs):
    """
    K·∫øt n·ªëi t·ªõi Ollama ƒëang ch·∫°y Local (tr√™n m√°y t√≠nh c·ªßa b·∫°n).
    M·∫∑c ƒë·ªãnh d√πng model 'llama3.2' (b·∫£n 3B) v√† c·ªïng 11434.
    """
    print(f"üîå ƒêang k·∫øt n·ªëi t·ªõi Ollama t·∫°i {base_url} v·ªõi model: {model_name}...")
    
    try:
        llm = Ollama(
            base_url=base_url,
            model=model_name,
            # C√°c tham s·ªë c·∫•u h√¨nh th√™m (n·∫øu c√≥)
            temperature=kwargs.get("temperature", 0.7),
            **kwargs
        )
        return llm
    except Exception as e:
        print(f"‚ùå L·ªói khi kh·ªüi t·∫°o Ollama: {e}")
        return None

# --- Ph·∫ßn n√†y ƒë·ªÉ test nhanh khi ch·∫°y tr·ª±c ti·∫øp file n√†y ---
if __name__ == "__main__":
    # Test th·ª≠ k·∫øt n·ªëi
    print("--- ƒêANG TEST K·∫æT N·ªêI OLLAMA ---")
    try:
        # G·ªçi h√†m kh·ªüi t·∫°o
        my_llm = get_ollama_llm() # M·∫∑c ƒë·ªãnh l√† llama3.2
        
        # Th·ª≠ h·ªèi m·ªôt c√¢u
        question = "Gi·∫£i th√≠ch RAG l√† g√¨ cho sinh vi√™n CNTT m·ªôt c√°ch ng·∫Øn g·ªçn."
        print(f"‚ùì C√¢u h·ªèi: {question}")
        
        response = my_llm.invoke(question)
        print("\nü§ñ Tr·∫£ l·ªùi:")
        print(response)
        print("\n‚úÖ TH√ÄNH C√îNG! Ollama ƒëang ho·∫°t ƒë·ªông t·ªët.")
    except Exception as e:
        print("\n‚ùå TH·∫§T B·∫†I! H√£y ki·ªÉm tra xem b·∫°n ƒë√£ b·∫≠t App Ollama ch∆∞a?")
        print(f"L·ªói chi ti·∫øt: {e}")